{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersonAttrubutes_InceptionResNetV2_100_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monimoyd/Predict_Personal_Attributes/blob/master/PersonAttrubutes_InceptionResNetV2_100_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!rm -rf resized\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46OvuswGJoQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_KERAS_BACKEND = None\n",
        "_KERAS_LAYERS = None\n",
        "_KERAS_MODELS = None\n",
        "_KERAS_UTILS = None\n",
        "\n",
        "\n",
        "def get_submodules_from_kwargs(kwargs):\n",
        "    backend = kwargs.get('backend', _KERAS_BACKEND)\n",
        "    layers = kwargs.get('layers', _KERAS_LAYERS)\n",
        "    models = kwargs.get('models', _KERAS_MODELS)\n",
        "    utils = kwargs.get('utils', _KERAS_UTILS)\n",
        "    for key in kwargs.keys():\n",
        "        if key not in ['backend', 'layers', 'models', 'utils']:\n",
        "            raise TypeError('Invalid keyword argument: %s', key)\n",
        "    return backend, layers, models, utils\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0EWm4L_JWWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Utilities for ImageNet data preprocessing & prediction decoding.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import json\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "from . import get_submodules_from_kwargs\n",
        "\n",
        "CLASS_INDEX = None\n",
        "CLASS_INDEX_PATH = ('https://storage.googleapis.com/download.tensorflow.org/'\n",
        "                    'data/imagenet_class_index.json')\n",
        "\n",
        "\n",
        "def _preprocess_numpy_input(x, data_format, mode, **kwargs):\n",
        "    \"\"\"Preprocesses a Numpy array encoding a batch of images.\n",
        "    # Arguments\n",
        "        x: Input array, 3D or 4D.\n",
        "        data_format: Data format of the image array.\n",
        "        mode: One of \"caffe\", \"tf\" or \"torch\".\n",
        "            - caffe: will convert the images from RGB to BGR,\n",
        "                then will zero-center each color channel with\n",
        "                respect to the ImageNet dataset,\n",
        "                without scaling.\n",
        "            - tf: will scale pixels between -1 and 1,\n",
        "                sample-wise.\n",
        "            - torch: will scale pixels between 0 and 1 and then\n",
        "                will normalize each channel with respect to the\n",
        "                ImageNet dataset.\n",
        "    # Returns\n",
        "        Preprocessed Numpy array.\n",
        "    \"\"\"\n",
        "    backend, _, _, _ = get_submodules_from_kwargs(kwargs)\n",
        "    if not issubclass(x.dtype.type, np.floating):\n",
        "        x = x.astype(K.floatx(), copy=False)\n",
        "\n",
        "    if mode == 'tf':\n",
        "        x /= 127.5\n",
        "        x -= 1.\n",
        "        return x\n",
        "\n",
        "    if mode == 'torch':\n",
        "        x /= 255.\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    else:\n",
        "        if data_format == 'channels_first':\n",
        "            # 'RGB'->'BGR'\n",
        "            if x.ndim == 3:\n",
        "                x = x[::-1, ...]\n",
        "            else:\n",
        "                x = x[:, ::-1, ...]\n",
        "        else:\n",
        "            # 'RGB'->'BGR'\n",
        "            x = x[..., ::-1]\n",
        "        mean = [103.939, 116.779, 123.68]\n",
        "        std = None\n",
        "\n",
        "    # Zero-center by mean pixel\n",
        "    if data_format == 'channels_first':\n",
        "        if x.ndim == 3:\n",
        "            x[0, :, :] -= mean[0]\n",
        "            x[1, :, :] -= mean[1]\n",
        "            x[2, :, :] -= mean[2]\n",
        "            if std is not None:\n",
        "                x[0, :, :] /= std[0]\n",
        "                x[1, :, :] /= std[1]\n",
        "                x[2, :, :] /= std[2]\n",
        "        else:\n",
        "            x[:, 0, :, :] -= mean[0]\n",
        "            x[:, 1, :, :] -= mean[1]\n",
        "            x[:, 2, :, :] -= mean[2]\n",
        "            if std is not None:\n",
        "                x[:, 0, :, :] /= std[0]\n",
        "                x[:, 1, :, :] /= std[1]\n",
        "                x[:, 2, :, :] /= std[2]\n",
        "    else:\n",
        "        x[..., 0] -= mean[0]\n",
        "        x[..., 1] -= mean[1]\n",
        "        x[..., 2] -= mean[2]\n",
        "        if std is not None:\n",
        "            x[..., 0] /= std[0]\n",
        "            x[..., 1] /= std[1]\n",
        "            x[..., 2] /= std[2]\n",
        "    return x\n",
        "\n",
        "\n",
        "def _preprocess_symbolic_input(x, data_format, mode, **kwargs):\n",
        "    \"\"\"Preprocesses a tensor encoding a batch of images.\n",
        "    # Arguments\n",
        "        x: Input tensor, 3D or 4D.\n",
        "        data_format: Data format of the image tensor.\n",
        "        mode: One of \"caffe\", \"tf\" or \"torch\".\n",
        "            - caffe: will convert the images from RGB to BGR,\n",
        "                then will zero-center each color channel with\n",
        "                respect to the ImageNet dataset,\n",
        "                without scaling.\n",
        "            - tf: will scale pixels between -1 and 1,\n",
        "                sample-wise.\n",
        "            - torch: will scale pixels between 0 and 1 and then\n",
        "                will normalize each channel with respect to the\n",
        "                ImageNet dataset.\n",
        "    # Returns\n",
        "        Preprocessed tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    backend, _, _, _ = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "    if mode == 'tf':\n",
        "        x /= 127.5\n",
        "        x -= 1.\n",
        "        return x\n",
        "\n",
        "    if mode == 'torch':\n",
        "        x /= 255.\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    else:\n",
        "        if data_format == 'channels_first':\n",
        "            # 'RGB'->'BGR'\n",
        "            if K.ndim(x) == 3:\n",
        "                x = x[::-1, ...]\n",
        "            else:\n",
        "                x = x[:, ::-1, ...]\n",
        "        else:\n",
        "            # 'RGB'->'BGR'\n",
        "            x = x[..., ::-1]\n",
        "        mean = [103.939, 116.779, 123.68]\n",
        "        std = None\n",
        "\n",
        "    mean_tensor = K.constant(-np.array(mean))\n",
        "\n",
        "    # Zero-center by mean pixel\n",
        "    if backend.dtype(x) != backend.dtype(mean_tensor):\n",
        "        x = backend.bias_add(\n",
        "            x, backend.cast(mean_tensor, backend.dtype(x)),\n",
        "            data_format=data_format)\n",
        "    else:\n",
        "        x = backend.bias_add(x, mean_tensor, data_format)\n",
        "    if std is not None:\n",
        "        x /= std\n",
        "    return x\n",
        "\n",
        "\n",
        "def preprocess_input(x, data_format=None, mode='caffe', **kwargs):\n",
        "    \"\"\"Preprocesses a tensor or Numpy array encoding a batch of images.\n",
        "    # Arguments\n",
        "        x: Input Numpy or symbolic tensor, 3D or 4D.\n",
        "            The preprocessed data is written over the input data\n",
        "            if the data types are compatible. To avoid this\n",
        "            behaviour, `numpy.copy(x)` can be used.\n",
        "        data_format: Data format of the image tensor/array.\n",
        "        mode: One of \"caffe\", \"tf\" or \"torch\".\n",
        "            - caffe: will convert the images from RGB to BGR,\n",
        "                then will zero-center each color channel with\n",
        "                respect to the ImageNet dataset,\n",
        "                without scaling.\n",
        "            - tf: will scale pixels between -1 and 1,\n",
        "                sample-wise.\n",
        "            - torch: will scale pixels between 0 and 1 and then\n",
        "                will normalize each channel with respect to the\n",
        "                ImageNet dataset.\n",
        "    # Returns\n",
        "        Preprocessed tensor or Numpy array.\n",
        "    # Raises\n",
        "        ValueError: In case of unknown `data_format` argument.\n",
        "    \"\"\"\n",
        "    backend, _, _, _ = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "    if data_format is None:\n",
        "        data_format = backend.image_data_format()\n",
        "    if data_format not in {'channels_first', 'channels_last'}:\n",
        "        raise ValueError('Unknown data_format ' + str(data_format))\n",
        "\n",
        "    if isinstance(x, np.ndarray):\n",
        "        return _preprocess_numpy_input(x, data_format=data_format,\n",
        "                                       mode=mode, **kwargs)\n",
        "    else:\n",
        "        return _preprocess_symbolic_input(x, data_format=data_format,\n",
        "                                          mode=mode, **kwargs)\n",
        "\n",
        "\n",
        "def decode_predictions(preds, top=5, **kwargs):\n",
        "    \"\"\"Decodes the prediction of an ImageNet model.\n",
        "    # Arguments\n",
        "        preds: Numpy tensor encoding a batch of predictions.\n",
        "        top: Integer, how many top-guesses to return.\n",
        "    # Returns\n",
        "        A list of lists of top class prediction tuples\n",
        "        `(class_name, class_description, score)`.\n",
        "        One list of tuples per sample in batch input.\n",
        "    # Raises\n",
        "        ValueError: In case of invalid shape of the `pred` array\n",
        "            (must be 2D).\n",
        "    \"\"\"\n",
        "    global CLASS_INDEX\n",
        "\n",
        "    backend, _, _, keras_utils = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "    if len(preds.shape) != 2 or preds.shape[1] != 1000:\n",
        "        raise ValueError('`decode_predictions` expects '\n",
        "                         'a batch of predictions '\n",
        "                         '(i.e. a 2D array of shape (samples, 1000)). '\n",
        "                         'Found array with shape: ' + str(preds.shape))\n",
        "    if CLASS_INDEX is None:\n",
        "        fpath = keras_utils.get_file(\n",
        "            'imagenet_class_index.json',\n",
        "            CLASS_INDEX_PATH,\n",
        "            cache_subdir='models',\n",
        "            file_hash='c2c37ea517e94d9795004a39431a14cb')\n",
        "        with open(fpath) as f:\n",
        "            CLASS_INDEX = json.load(f)\n",
        "    results = []\n",
        "    for pred in preds:\n",
        "        top_indices = pred.argsort()[-top:][::-1]\n",
        "        result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]\n",
        "        result.sort(key=lambda x: x[2], reverse=True)\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "\n",
        "def _obtain_input_shape(input_shape,\n",
        "                        default_size,\n",
        "                        min_size,\n",
        "                        data_format,\n",
        "                        require_flatten,\n",
        "                        weights=None):\n",
        "    \"\"\"Internal utility to compute/validate a model's input shape.\n",
        "    # Arguments\n",
        "        input_shape: Either None (will return the default network input shape),\n",
        "            or a user-provided shape to be validated.\n",
        "        default_size: Default input width/height for the model.\n",
        "        min_size: Minimum input width/height accepted by the model.\n",
        "        data_format: Image data format to use.\n",
        "        require_flatten: Whether the model is expected to\n",
        "            be linked to a classifier via a Flatten layer.\n",
        "        weights: One of `None` (random initialization)\n",
        "            or 'imagenet' (pre-training on ImageNet).\n",
        "            If weights='imagenet' input channels must be equal to 3.\n",
        "    # Returns\n",
        "        An integer shape tuple (may include None entries).\n",
        "    # Raises\n",
        "        ValueError: In case of invalid argument values.\n",
        "    \"\"\"\n",
        "    if weights != 'imagenet' and input_shape and len(input_shape) == 3:\n",
        "        if data_format == 'channels_first':\n",
        "            if input_shape[0] not in {1, 3}:\n",
        "                warnings.warn(\n",
        "                    'This model usually expects 1 or 3 input channels. '\n",
        "                    'However, it was passed an input_shape with ' +\n",
        "                    str(input_shape[0]) + ' input channels.')\n",
        "            default_shape = (input_shape[0], default_size, default_size)\n",
        "        else:\n",
        "            if input_shape[-1] not in {1, 3}:\n",
        "                warnings.warn(\n",
        "                    'This model usually expects 1 or 3 input channels. '\n",
        "                    'However, it was passed an input_shape with ' +\n",
        "                    str(input_shape[-1]) + ' input channels.')\n",
        "            default_shape = (default_size, default_size, input_shape[-1])\n",
        "    else:\n",
        "        if data_format == 'channels_first':\n",
        "            default_shape = (3, default_size, default_size)\n",
        "        else:\n",
        "            default_shape = (default_size, default_size, 3)\n",
        "    if weights == 'imagenet' and require_flatten:\n",
        "        if input_shape is not None:\n",
        "            if input_shape != default_shape:\n",
        "                raise ValueError('When setting `include_top=True` '\n",
        "                                 'and loading `imagenet` weights, '\n",
        "                                 '`input_shape` should be ' +\n",
        "                                 str(default_shape) + '.')\n",
        "        return default_shape\n",
        "    if input_shape:\n",
        "        if data_format == 'channels_first':\n",
        "            if input_shape is not None:\n",
        "                if len(input_shape) != 3:\n",
        "                    raise ValueError(\n",
        "                        '`input_shape` must be a tuple of three integers.')\n",
        "                if input_shape[0] != 3 and weights == 'imagenet':\n",
        "                    raise ValueError('The input must have 3 channels; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "                if ((input_shape[1] is not None and input_shape[1] < min_size) or\n",
        "                   (input_shape[2] is not None and input_shape[2] < min_size)):\n",
        "                    raise ValueError('Input size must be at least ' +\n",
        "                                     str(min_size) + 'x' + str(min_size) +\n",
        "                                     '; got `input_shape=' +\n",
        "                                     str(input_shape) + '`')\n",
        "        else:\n",
        "            if input_shape is not None:\n",
        "                if len(input_shape) != 3:\n",
        "                    raise ValueError(\n",
        "                        '`input_shape` must be a tuple of three integers.')\n",
        "                if input_shape[-1] != 3 and weights == 'imagenet':\n",
        "                    raise ValueError('The input must have 3 channels; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "                if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
        "                   (input_shape[1] is not None and input_shape[1] < min_size)):\n",
        "                    raise ValueError('Input size must be at least ' +\n",
        "                                     str(min_size) + 'x' + str(min_size) +\n",
        "                                     '; got `input_shape=' +\n",
        "                                     str(input_shape) + '`')\n",
        "    else:\n",
        "        if require_flatten:\n",
        "            input_shape = default_shape\n",
        "        else:\n",
        "            if data_format == 'channels_first':\n",
        "                input_shape = (3, None, None)\n",
        "            else:\n",
        "                input_shape = (None, None, 3)\n",
        "    if require_flatten:\n",
        "        if None in input_shape:\n",
        "            raise ValueError('If `include_top` is True, '\n",
        "                             'you should specify a static `input_shape`. '\n",
        "                             'Got `input_shape=' + str(input_shape) + '`')\n",
        "    return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G-qWKIoeNhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "from keras.layers import Conv2D,Lambda,Activation,MaxPool2D,Concatenate,GlobalAveragePooling2D,AveragePooling2D,Dense,BatchNormalization,MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "#from . import get_submodules_from_kwargs\n",
        "#from . import imagenet_utils\n",
        "#from .imagenet_utils import decode_predictions\n",
        "#from .imagenet_utils import _obtain_input_shape\n",
        "\n",
        "\n",
        "\n",
        "#backend = None\n",
        "#layers = None\n",
        "#models = None\n",
        "#keras_utils = None\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_input(x, **kwargs):\n",
        "    \"\"\"Preprocesses a numpy array encoding a batch of images.\n",
        "    # Arguments\n",
        "        x: a 4D numpy array consists of RGB values within [0, 255].\n",
        "    # Returns\n",
        "        Preprocessed array.\n",
        "    \"\"\"\n",
        "    return imagenet_utils.preprocess_input(x, mode='tf', **kwargs)\n",
        "\n",
        "\n",
        "def conv2d_bn(x,\n",
        "              filters,\n",
        "              kernel_size,\n",
        "              strides=1,\n",
        "              padding='same',\n",
        "              activation='relu',\n",
        "              use_bias=False,\n",
        "              name=None):\n",
        "    \"\"\"Utility function to apply conv + BN.\n",
        "    # Arguments\n",
        "        x: input tensor.\n",
        "        filters: filters in `Conv2D`.\n",
        "        kernel_size: kernel size as in `Conv2D`.\n",
        "        strides: strides in `Conv2D`.\n",
        "        padding: padding mode in `Conv2D`.\n",
        "        activation: activation in `Conv2D`.\n",
        "        use_bias: whether to use a bias in `Conv2D`.\n",
        "        name: name of the ops; will become `name + '_ac'` for the activation\n",
        "            and `name + '_bn'` for the batch norm layer.\n",
        "    # Returns\n",
        "        Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
        "    \"\"\"\n",
        "    x = Conv2D(filters,\n",
        "                      kernel_size,\n",
        "                      strides=strides,\n",
        "                      padding=padding,\n",
        "                      use_bias=use_bias,\n",
        "                      name=name)(x)\n",
        "    if not use_bias:\n",
        "        bn_axis = 3\n",
        "        bn_name = None if name is None else name + '_bn'\n",
        "        x = BatchNormalization(axis=bn_axis,\n",
        "                                      scale=False,\n",
        "                                      name=bn_name)(x)\n",
        "    if activation is not None:\n",
        "        ac_name = None if name is None else name + '_ac'\n",
        "        x = Activation(activation, name=ac_name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def inception_resnet_block(x, scale, block_type, block_idx, activation='relu'):\n",
        "    \"\"\"Adds a Inception-ResNet block.\n",
        "    This function builds 3 types of Inception-ResNet blocks mentioned\n",
        "    in the paper, controlled by the `block_type` argument (which is the\n",
        "    block name used in the official TF-slim implementation):\n",
        "        - Inception-ResNet-A: `block_type='block35'`\n",
        "        - Inception-ResNet-B: `block_type='block17'`\n",
        "        - Inception-ResNet-C: `block_type='block8'`\n",
        "    # Arguments\n",
        "        x: input tensor.\n",
        "        scale: scaling factor to scale the residuals (i.e., the output of\n",
        "            passing `x` through an inception module) before adding them\n",
        "            to the shortcut branch.\n",
        "            Let `r` be the output from the residual branch,\n",
        "            the output of this block will be `x + scale * r`.\n",
        "        block_type: `'block35'`, `'block17'` or `'block8'`, determines\n",
        "            the network structure in the residual branch.\n",
        "        block_idx: an `int` used for generating layer names.\n",
        "            The Inception-ResNet blocks\n",
        "            are repeated many times in this network.\n",
        "            We use `block_idx` to identify\n",
        "            each of the repetitions. For example,\n",
        "            the first Inception-ResNet-A block\n",
        "            will have `block_type='block35', block_idx=0`,\n",
        "            and the layer names will have\n",
        "            a common prefix `'block35_0'`.\n",
        "        activation: activation function to use at the end of the block\n",
        "            (see [activations](../activations.md)).\n",
        "            When `activation=None`, no activation is applied\n",
        "            (i.e., \"linear\" activation: `a(x) = x`).\n",
        "    # Returns\n",
        "        Output tensor for the block.\n",
        "    # Raises\n",
        "        ValueError: if `block_type` is not one of `'block35'`,\n",
        "            `'block17'` or `'block8'`.\n",
        "    \"\"\"\n",
        "    if block_type == 'block35':\n",
        "        branch_0 = conv2d_bn(x, 32, 1)\n",
        "        branch_1 = conv2d_bn(x, 32, 1)\n",
        "        branch_1 = conv2d_bn(branch_1, 32, 3)\n",
        "        branch_2 = conv2d_bn(x, 32, 1)\n",
        "        branch_2 = conv2d_bn(branch_2, 48, 3)\n",
        "        branch_2 = conv2d_bn(branch_2, 64, 3)\n",
        "        branches = [branch_0, branch_1, branch_2]\n",
        "    elif block_type == 'block17':\n",
        "        branch_0 = conv2d_bn(x, 192, 1)\n",
        "        branch_1 = conv2d_bn(x, 128, 1)\n",
        "        branch_1 = conv2d_bn(branch_1, 160, [1, 7])\n",
        "        branch_1 = conv2d_bn(branch_1, 192, [7, 1])\n",
        "        branches = [branch_0, branch_1]\n",
        "    elif block_type == 'block8':\n",
        "        branch_0 = conv2d_bn(x, 192, 1)\n",
        "        branch_1 = conv2d_bn(x, 192, 1)\n",
        "        branch_1 = conv2d_bn(branch_1, 224, [1, 3])\n",
        "        branch_1 = conv2d_bn(branch_1, 256, [3, 1])\n",
        "        branches = [branch_0, branch_1]\n",
        "    else:\n",
        "        raise ValueError('Unknown Inception-ResNet block type. '\n",
        "                         'Expects \"block35\", \"block17\" or \"block8\", '\n",
        "                         'but got: ' + str(block_type))\n",
        "\n",
        "    block_name = block_type + '_' + str(block_idx)\n",
        "    channel_axis = 3\n",
        "    mixed = Concatenate(\n",
        "        axis=channel_axis, name=block_name + '_mixed')(branches)\n",
        "    up = conv2d_bn(mixed,\n",
        "                   K.int_shape(x)[channel_axis],\n",
        "                   1,\n",
        "                   activation=None,\n",
        "                   use_bias=True,\n",
        "                   name=block_name + '_conv')\n",
        "\n",
        "    x = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n",
        "                      output_shape=K.int_shape(x)[1:],\n",
        "                      arguments={'scale': scale},\n",
        "                      name=block_name)([x, up])\n",
        "    if activation is not None:\n",
        "        x = Activation(activation, name=block_name + '_ac')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def InceptionResNetV2(include_top=True,\n",
        "                      input_tensor=None,\n",
        "                      input_shape=None,\n",
        "                      pooling=None,\n",
        "                      classes=1000,\n",
        "                      **kwargs):\n",
        "    \"\"\"Instantiates the Inception-ResNet v2 architecture.\n",
        "    \n",
        "    # Arguments\n",
        "        include_top: whether to include the fully-connected\n",
        "            layer at the top of the network.\n",
        "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
        "            to use as image input for the model.\n",
        "        input_shape: optional shape tuple, only to be specified\n",
        "            if `include_top` is `False` (otherwise the input shape\n",
        "            has to be `(299, 299, 3)` (with `'channels_last'` data format)\n",
        "            or `(3, 299, 299)` (with `'channels_first'` data format).\n",
        "            It should have exactly 3 inputs channels,\n",
        "            and width and height should be no smaller than 75.\n",
        "            E.g. `(150, 150, 3)` would be one valid value.\n",
        "        pooling: Optional pooling mode for feature extraction\n",
        "            when `include_top` is `False`.\n",
        "            - `None` means that the output of the model will be\n",
        "                the 4D tensor output of the last convolutional block.\n",
        "            - `'avg'` means that global average pooling\n",
        "                will be applied to the output of the\n",
        "                last convolutional block, and thus\n",
        "                the output of the model will be a 2D tensor.\n",
        "            - `'max'` means that global max pooling will be applied.\n",
        "        classes: optional number of classes to classify images\n",
        "            into, only to be specified if `include_top` is `True`, and\n",
        "            if no `weights` argument is specified.\n",
        "    # Returns\n",
        "        A Keras `Model` instance.\n",
        "    # Raises\n",
        "        ValueError: in case of invalid argument for `weights`,\n",
        "            or invalid input shape.\n",
        "    \"\"\"\n",
        "    global backend, layers, models, keras_utils\n",
        "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "    img_input = input_tensor\n",
        "                \n",
        "\n",
        "    # Stem block: 35 x 35 x 192\n",
        "    x = conv2d_bn(img_input, 32, 3, strides=2, padding='valid')\n",
        "    x = conv2d_bn(x, 32, 3, padding='valid')\n",
        "    x = conv2d_bn(x, 64, 3)\n",
        "    x = MaxPooling2D(3, strides=2)(x)\n",
        "    x = conv2d_bn(x, 80, 1, padding='valid')\n",
        "    x = conv2d_bn(x, 192, 3, padding='valid')\n",
        "    x = MaxPooling2D(3, strides=2)(x)\n",
        "\n",
        "    # Mixed 5b (Inception-A block): 35 x 35 x 320\n",
        "    branch_0 = conv2d_bn(x, 96, 1)\n",
        "    branch_1 = conv2d_bn(x, 48, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 64, 5)\n",
        "    branch_2 = conv2d_bn(x, 64, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 96, 3)\n",
        "    branch_2 = conv2d_bn(branch_2, 96, 3)\n",
        "    branch_pool = AveragePooling2D(3, strides=1, padding='same')(x)\n",
        "    branch_pool = conv2d_bn(branch_pool, 64, 1)\n",
        "    branches = [branch_0, branch_1, branch_2, branch_pool]\n",
        "    channel_axis = 3\n",
        "    x = Concatenate(axis=channel_axis, name='mixed_5b')(branches)\n",
        "\n",
        "    # 10x block35 (Inception-ResNet-A block): 35 x 35 x 320\n",
        "    for block_idx in range(1, 11):\n",
        "        x = inception_resnet_block(x,\n",
        "                                   scale=0.17,\n",
        "                                   block_type='block35',\n",
        "                                   block_idx=block_idx)\n",
        "\n",
        "    # Mixed 6a (Reduction-A block): 17 x 17 x 1088\n",
        "    branch_0 = conv2d_bn(x, 384, 3, strides=2, padding='valid')\n",
        "    branch_1 = conv2d_bn(x, 256, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 256, 3)\n",
        "    branch_1 = conv2d_bn(branch_1, 384, 3, strides=2, padding='valid')\n",
        "    branch_pool = MaxPooling2D(3, strides=2, padding='valid')(x)\n",
        "    branches = [branch_0, branch_1, branch_pool]\n",
        "    x = Concatenate(axis=channel_axis, name='mixed_6a')(branches)\n",
        "\n",
        "    # 20x block17 (Inception-ResNet-B block): 17 x 17 x 1088\n",
        "    for block_idx in range(1, 21):\n",
        "        x = inception_resnet_block(x,\n",
        "                                   scale=0.1,\n",
        "                                   block_type='block17',\n",
        "                                   block_idx=block_idx)\n",
        "\n",
        "    # Mixed 7a (Reduction-B block): 8 x 8 x 2080\n",
        "    branch_0 = conv2d_bn(x, 256, 1)\n",
        "    branch_0 = conv2d_bn(branch_0, 384, 3, strides=2, padding='valid')\n",
        "    branch_1 = conv2d_bn(x, 256, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 288, 3, strides=2, padding='valid')\n",
        "    branch_2 = conv2d_bn(x, 256, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 288, 3)\n",
        "    branch_2 = conv2d_bn(branch_2, 320, 3, strides=2, padding='valid')\n",
        "    branch_pool = MaxPooling2D(3, strides=2, padding='valid')(x)\n",
        "    branches = [branch_0, branch_1, branch_2, branch_pool]\n",
        "    x = Concatenate(axis=channel_axis, name='mixed_7a')(branches)\n",
        "\n",
        "    # 10x block8 (Inception-ResNet-C block): 8 x 8 x 2080\n",
        "    for block_idx in range(1, 10):\n",
        "        x = inception_resnet_block(x,\n",
        "                                   scale=0.2,\n",
        "                                   block_type='block8',\n",
        "                                   block_idx=block_idx)\n",
        "    x = inception_resnet_block(x,\n",
        "                               scale=1.,\n",
        "                               activation=None,\n",
        "                               block_type='block8',\n",
        "                               block_idx=10)\n",
        "\n",
        "    # Final convolution block: 8 x 8 x 1536\n",
        "    x = conv2d_bn(x, 1536, 1, name='conv_7b')\n",
        "\n",
        "    if include_top:\n",
        "        # Classification block\n",
        "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "        elif pooling == 'max':\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "   \n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = Model(inputs, x, name='inception_resnet_v2')\n",
        "\n",
        "    \n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.applications.inception_v3 import InceptionV3\n",
        "#from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "\n",
        "\n",
        "backbone = InceptionResNetV2(\n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "import tensorflow as tf\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=0.2)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=custom_loss, \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "#model.compile(optimizer, loss=custom_loss)\n",
        "#model.compile(\n",
        "#    optimizer=opt,\n",
        "#    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        " #   metrics=[\"accuracy\"]\n",
        "#)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=25,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc11oDCcw54U",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}